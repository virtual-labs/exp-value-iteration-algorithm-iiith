{
  "version": 2.0,
  "questions": [
    {
      "question": "Value Iteration is an example which programming method",
      "answers": {
        "a": "Greedy",
        "b": "Dynamic Programming",
        "c": "Object-oriented programming",
        "d": "None of them"
      },
      "explanations": {
        "a": "",
        "b": "Value Iteration is a dynamic programming method to find solutions for MDPs.",
        "c": "",
        "d": ""
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "Which of the following is true regarding Value Iteration?",
      "answers": {
        "a": "The utility of many states do not change in one iteration, but the process has to continue as long as there is change in some states",
        "b": "Discount factor effects the convergence of the algorithm",
        "c": "Sometimes the corresponding policy has already converged to optimal, but the values have not converged and therefore we have to continue the value iteration process",
        "d": "All of the above"
      },
      "explanations": {
        "a": "All of the above statements are true",
        "b": "",
        "c": "",
        "d": ""
      },
      "correctAnswer": "d",
      "difficulty": "beginner"
    },
    {
      "question": "Which of the following is 'false' regarding optimal-value function?",
      "answers": {
        "a": "It is the maximum Value function over all policies",
        "b": "For every finite MDP, optimal-value function is unique",
        "c": "For a finite MDP, optimal-value function is not guaranteed to be unique",
        "d": "All of the above"
      },
      "explanations": {
        "a": "The Bellman optimality equation is actually a system of equations, one for each state, so if there are N states, then there are N equations in N unknowns. If the dynamics  of the environment are known, then in principle one can solve this system of equations for the optimal value function using any one of a variety of methods for solving systems of nonlinear equations. All optimal policies share the same optimal state-value function.",
        "b": "",
        "c": "",
        "d": ""
      },
      "correctAnswer": "c",
      "difficulty": "intermediate"
    },
    {
      "question": "Adding constants to reward effects optimal policy in :",
      "answers": {
        "a": "Both Episodic tasks and continuous tasks",
        "b": "Continuous tasks only",
        "c": "Episodic tasks only",
        "d": "None of them"
      },
      "explanations": {
        "a": "",
        "b": "",
        "c": "Adding constants to rewards in episodic can have effect on longer paths in the policy. Whereas adding constants to rewards in continuous tasks has no effect on optimal policy, as long as the relative differences between rewards remain the same, the set of optimal policies is the same.",
        "d": ""
      },
      "correctAnswer": "c",
      "difficulty": "advanced"
    },
    {
      "question": "Value Iteration always find the optimal policy, when run to convergence",
      "answers": {
        "a": "True",
        "b": "False",
        "c": "Cannot Say",
        "d": "None of them"
      },
      "explanations": {
        "a": "This is a result of the Bellman backups being a contraction, as discussed.",
        "b": "",
        "c": "",
        "d": ""
      },
      "correctAnswer": "a",
      "difficulty": "intermediate"
    },
    {
      "question": "Optimal policy can be reached before all the Vk(s) (value function of state) reaches their optimal value.",
      "answers": {
        "a": "True, after certain stage policy doesnot depend upon the utility values of state.",
        "b": "True, because policy can remain the same while the utility value of state changes",
        "c": "False, only after all the states reach their optimal utility values the optimal policy can be declared",
        "d": "None of them"
      },
      "explanations": {
        "a": "",
        "b": "Even though utility values of states are changing, the optimal action to take in a state can remain the same",
        "c": "",
        "d": ""
      },
      "correctAnswer": "b",
      "difficulty": "advanced"
    }
    

  ]
}
