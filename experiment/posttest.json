{
  "version": 2.0,
  "questions": [
    {
      "question": "Which of the following is true regarding Value Iteration?",
      "answers": {
        "a": "The utility of many states do not change in one iteration, but the process has to continue as long as there is change in some states",
        "b": "Discount factor effects the convergence of the algorithm",
        "c": "Sometimes the corresponding policy has already converged to optimal, but the values have not converged and therefore we have to continue the value iteration process",
        "d": "All of the above"
      },
      "explanations": {
        "a": "All of the above statements are true",
        "b": "",
        "c": "",
        "d": ""
      },
      "correctAnswer": "d",
      "difficulty": "intermediate"
    },
    {
      "question": "Which of the following is 'false' regarding optimal-value function?",
      "answers": {
        "a": "It is the maximum Value function over all policies",
        "b": "For every finite MDP, optimal-value function is unique",
        "c": "For a finite MDP, optimal-value function is not guaranteed to be unique",
        "d": "All of the above"
      },
      "explanations": {
        "a": "The Bellman optimality equation is actually a system of equations, one for each state, so if there are N states, then there are N equations in N unknowns. If the dynamics  of the environment are known, then in principle one can solve this system of equations for the optimal value function using any one of a variety of methods for solving systems of nonlinear equations. All optimal policies share the same optimal state-value function.",
        "b": "",
        "c": "",
        "d": ""
      },
      "correctAnswer": "c",
      "difficulty": "advanced"
    },
    {
      "question": "Adding constants to reward effects optimal policy in :",
      "answers": {
        "a": "Both Episodic tasks and continuous tasks",
        "b": "Continuous tasks only",
        "c": "Episodic tasks only",
        "d": "None of them"
      },
      "explanations": {
        "a": "",
        "b": "",
        "c": "Adding constants to rewards in episodic can have effect on longer paths in the policy. Whereas adding constants to rewards in continuous tasks has no effect on optimal policy, as long as the relative differences between rewards remain the same, the set of optimal policies is the same.",
        "d": ""
      },
      "correctAnswer": "c",
      "difficulty": "advanced"
    }
  ]
}
